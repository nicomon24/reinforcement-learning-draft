{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on atari's TBD\n",
    "We are going to implement DQN on the atari game TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-20 15:30:53,213] Making new env: Breakout-v0\n",
      "[2017-10-20 15:30:53,404] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/nicom/Documents/Projects/openai/atari/tmp/dqn-results')\n",
      "[2017-10-20 15:30:53,405] Clearing 8 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[42, 742738649]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "# Create env\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "# Setup wrapper\n",
    "outdir = 'tmp/dqn-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "# Set seed for replication \n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "We are going to scale and crop the image to make our learning easier (without losing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-20 15:30:55,163] Starting new video recorder writing to /Users/nicom/Documents/Projects/openai/atari/tmp/dqn-results/openaigym.video.1.10814.video000000.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x117472f28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUYZfapThlxr1v5V/Rg9AphTVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1170a7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.misc import imresize\n",
    "\n",
    "IM_WIDTH = 80\n",
    "IM_HEIGHT = 80\n",
    "\n",
    "def preprocess_screen(observation):\n",
    "    # Crop image\n",
    "    observation = observation[31:194]\n",
    "    # Apply grayscale\n",
    "    observation = observation.mean(axis=2)\n",
    "    # Normalize 0-1\n",
    "    observation = observation / 255\n",
    "    # Scale to 80x80\n",
    "    observation = imresize(observation, size=(IM_HEIGHT, IM_WIDTH), interp='nearest')\n",
    "    return observation\n",
    "\n",
    "obs = env.reset()\n",
    "plt.imshow(preprocess_screen(obs), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the DQN model using layers this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self, K, gamma, scope):\n",
    "        self.K = K\n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(scope):\n",
    "            # Placeholders\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, IM_HEIGHT, IM_WIDTH, 4), name='X')\n",
    "            self.G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
    "            self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
    "            # Convolution\n",
    "            self.c1 = tf.contrib.layers.conv2d(inputs=self.X, num_outputs=16, kernel_size=8, stride=4)\n",
    "            self.c2 = tf.contrib.layers.conv2d(inputs=self.X, num_outputs=32, kernel_size=4, stride=2)\n",
    "            self.flatted = tf.contrib.layers.flatten(inputs=self.c2)\n",
    "            # Fully connected\n",
    "            self.d1 = tf.contrib.layers.fully_connected(inputs=self.flatted, num_outputs=256)\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(inputs=self.d1, num_outputs=K, activation_fn=None)\n",
    "            # Cost and train\n",
    "            selected_action_values = tf.reduce_sum(\n",
    "                self.output * tf.one_hot(self.actions, K),\n",
    "                reduction_indices=[1]\n",
    "            )\n",
    "            cost = tf.reduce_mean(tf.square(self.G - selected_action_values))\n",
    "            self.train_op = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6).minimize(cost)\n",
    "            self.cost = cost\n",
    "        \n",
    "    def copy_from(self, other):\n",
    "        mine = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
    "        mine = sorted(mine, key=lambda v: v.name)\n",
    "        theirs = [t for t in tf.trainable_variables() if t.name.startswith(other.scope)]\n",
    "        theirs = sorted(theirs, key=lambda v: v.name)\n",
    "\n",
    "        ops = []\n",
    "        for p, q in zip(mine, theirs):\n",
    "            actual = self.session.run(q)\n",
    "            op = p.assign(actual)\n",
    "            ops.append(op)\n",
    "\n",
    "        self.session.run(ops)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: states})\n",
    "    \n",
    "    def update(self, states, actions, targets):\n",
    "        c, _ = self.session.run([self.cost, self.train_op],\n",
    "              feed_dict={\n",
    "                self.X: states,\n",
    "                self.G: targets,\n",
    "                self.actions: actions\n",
    "              })\n",
    "        return c\n",
    "\n",
    "    def sample_action(self, x, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([x])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a queue to store the last 4 frames as the state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StateQueue:\n",
    "    \n",
    "    def __init__(self, initial_state, n = 4):\n",
    "        self.states = deque([initial_state] * n, maxlen = n)\n",
    "        \n",
    "    def add_state(self, state):\n",
    "        self.states.append(state)\n",
    "        \n",
    "    def get_state(self):\n",
    "        return np.transpose(np.array(self.states), [1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_one(env, experience_replay_buffer, model, tmodel, gamma, batch_size, eps, d_eps, min_eps):\n",
    "    obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "s = StateQueue(np.zeros((3,3)))\n",
    "print(s.get_state().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

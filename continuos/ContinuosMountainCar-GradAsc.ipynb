{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient in continuous action space using TF\n",
    "We will use TF to apply PG method to MountainCarContinuos environment. This time we will use gradient ascent instead of Hill Climb random search for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from q_learning import plot_running_avg, FeatureTransformer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='once'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the environment (CartPole V0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-18 13:10:06,888] Making new env: MountainCarContinuous-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCarContinuous-v0\").env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a layer of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class HiddenLayer:\n",
    "    def __init__(self, M1, M2, f=tf.nn.tanh, use_bias=True, zeros=False):\n",
    "        if zeros:\n",
    "            W = np.zeros((M1, M2)).astype(np.float32)\n",
    "            self.W = tf.Variable(W)\n",
    "        else:\n",
    "            self.W = tf.Variable(tf.random_normal(shape=(M1, M2)))\n",
    "            \n",
    "        self.params = [self.W]\n",
    "        \n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        if use_bias:\n",
    "            self.b = tf.Variable(np.zeros(M2).astype(np.float32))\n",
    "            self.params.append(self.b)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.use_bias:\n",
    "            a = tf.matmul(X, self.W) + self.b\n",
    "        else:\n",
    "            a = tf.matmul(X, self.W)\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we create the policy model. This time we will have 2 outputs, the mean and the var of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyModel:\n",
    "    def __init__(self, D, ft, hidden_layer_sizes=[]):\n",
    "        self.ft = ft\n",
    "\n",
    "        ##### hidden layers #####\n",
    "        M1 = D\n",
    "        self.hidden_layers = []\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.hidden_layers.append(layer)\n",
    "            M1 = M2\n",
    "\n",
    "        # final layer mean\n",
    "        self.mean_layer = HiddenLayer(M1, 1, lambda x: x, use_bias=False, zeros=True)\n",
    "\n",
    "        # final layer variance\n",
    "        self.stdv_layer = HiddenLayer(M1, 1, tf.nn.softplus, use_bias=False, zeros=False)\n",
    "\n",
    "        # inputs and targets\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        self.actions = tf.placeholder(tf.float32, shape=(None,), name='actions')\n",
    "        self.advantages = tf.placeholder(tf.float32, shape=(None,), name='advantages')\n",
    "\n",
    "        # get final hidden layer\n",
    "        Z = self.X\n",
    "        for layer in self.hidden_layers:\n",
    "            Z = layer.forward(Z)\n",
    "\n",
    "        # calculate output and cost\n",
    "        mean = self.mean_layer.forward(Z)\n",
    "        stdv = self.stdv_layer.forward(Z) + 1e-5 # smoothing\n",
    "\n",
    "        # make them 1-D\n",
    "        mean = tf.reshape(mean, [-1])\n",
    "        stdv = tf.reshape(stdv, [-1]) \n",
    "\n",
    "        norm = tf.contrib.distributions.Normal(mean, stdv)\n",
    "        self.predict_op = tf.clip_by_value(norm.sample(), -1, 1)\n",
    "\n",
    "        log_probs = norm.log_prob(self.actions)\n",
    "        cost = -tf.reduce_sum(self.advantages * log_probs + 0.1*norm.entropy())\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def partial_fit(self, X, actions, advantages):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "\n",
    "        actions = np.atleast_1d(actions)\n",
    "        advantages = np.atleast_1d(advantages)\n",
    "        self.session.run(\n",
    "          self.train_op,\n",
    "          feed_dict={\n",
    "            self.X: X,\n",
    "            self.actions: actions,\n",
    "            self.advantages: advantages,\n",
    "          }\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: X})\n",
    "\n",
    "    def sample_action(self, X):\n",
    "        p = self.predict(X)[0]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are not using Hill Climb anymore, we also need to model the Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueModel:\n",
    "    def __init__(self, D, ft, hidden_layer_sizes=[]):\n",
    "        self.ft = ft\n",
    "        self.costs = []\n",
    "\n",
    "        # create the graph\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "\n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, 1, lambda x: x)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        # inputs and targets\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        self.Y = tf.placeholder(tf.float32, shape=(None,), name='Y')\n",
    "\n",
    "        # calculate output and cost\n",
    "        Z = self.X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        Y_hat = tf.reshape(Z, [-1]) # the output\n",
    "        self.predict_op = Y_hat\n",
    "\n",
    "        cost = tf.reduce_sum(tf.square(self.Y - Y_hat))\n",
    "        self.cost = cost\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-1).minimize(cost)\n",
    "\n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def partial_fit(self, X, Y):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        Y = np.atleast_1d(Y)\n",
    "        self.session.run(self.train_op, feed_dict={self.X: X, self.Y: Y})\n",
    "        cost = self.session.run(self.cost, feed_dict={self.X: X, self.Y: Y})\n",
    "        self.costs.append(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: X})        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the play_one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_one_td(env, pmodel, vmodel, gamma, render = False):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    frames = []\n",
    "\n",
    "    while not done and iters < 2000:\n",
    "        # if we reach 2000, just quit, don't want this going forever\n",
    "        # the 200 limit seems a bit early\n",
    "        if render:\n",
    "            frames.append(env.render(mode = 'rgb_array'))\n",
    "        \n",
    "        action = pmodel.sample_action(observation)\n",
    "        prev_observation = observation\n",
    "        # oddly, the mountain car environment requires the action to be in\n",
    "        # an object where the actual action is stored in object[0]\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        totalreward += reward\n",
    "        # Update models\n",
    "        V_next = vmodel.predict(observation)\n",
    "        G = reward + gamma * V_next\n",
    "        advantage = G - vmodel.predict(prev_observation)\n",
    "        pmodel.partial_fit(prev_observation, action, advantage)\n",
    "        vmodel.partial_fit(prev_observation, G)\n",
    "        iters += 1\n",
    "    return totalreward, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: -172.5 num steps: 2000 avg reward (last 100): -172.5\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 1 total reward: -184.4 num steps: 2000 avg reward (last 100): -178.5\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 2 total reward: -188.1 num steps: 2000 avg reward (last 100): -181.7\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 3 total reward: -190.0 num steps: 2000 avg reward (last 100): -183.8\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 4 total reward: -192.0 num steps: 2000 avg reward (last 100): -185.4\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 5 total reward: -192.0 num steps: 2000 avg reward (last 100): -186.5\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 6 total reward: -193.6 num steps: 2000 avg reward (last 100): -187.5\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 7 total reward: -192.3 num steps: 2000 avg reward (last 100): -188.1\n",
      "avg reward for last 100 episodes: nan\n",
      "episode: 8 total reward: -194.3 num steps: 2000 avg reward (last 100): -188.8\n",
      "avg reward for last 100 episodes: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b89d349af853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtotalreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_td\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtotalrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8bd7408f11c8>\u001b[0m in \u001b[0;36mplay_one_td\u001b[0;34m(env, pmodel, vmodel, gamma, render)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0miters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotalreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d03e79bda99b>\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicom/Documents/Projects/openai/continuos/q_learning.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# print \"observations:\", observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# assert(len(scaled.shape) == 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicom/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    644\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[1;32m    645\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicom/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" by %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mestimator_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n",
      "\u001b[0;32m/Users/nicom/anaconda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36misspmatrix\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[0missparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft = FeatureTransformer(env, n_components=100)\n",
    "D = ft.dimensions\n",
    "pmodel = PolicyModel(D, ft, [])\n",
    "vmodel = ValueModel(D, ft, [])\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.InteractiveSession()\n",
    "session.run(init)\n",
    "pmodel.set_session(session)\n",
    "vmodel.set_session(session)\n",
    "gamma = 0.95\n",
    "\n",
    "N = 50\n",
    "totalrewards = np.empty(N)\n",
    "costs = np.empty(N)\n",
    "for n in range(N):\n",
    "    totalreward, num_steps = play_one_td(env, pmodel, vmodel, gamma)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 1 == 0:\n",
    "        print(\"episode:\", n, \"total reward: %.1f\" % totalreward, \"num steps: %d\" % num_steps, \"avg reward (last 100): %.1f\" % totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "\n",
    "plt.plot(totalrewards)\n",
    "plt.title(\"Rewards\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
